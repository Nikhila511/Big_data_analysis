{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harika Reddy Kumbum - 50248882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sai Nikhila Muthyala - 50245184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to collect tweets from twitter and store them in csv and txt files\n",
    "\n",
    "library(twitteR)\n",
    "consumer_key = \"***************************\"\n",
    "consumer_secret = \"****************************************\"\n",
    "setup_twitter_oauth(consumer_key, consumer_secret, access_token=NULL, access_secret=NULL )\n",
    "tweets <- searchTwitter(\"cambridge analytica\", n=20000, lang=\"en\", since = '2018-03-28', until = '2018-03-29')\n",
    "df <- do.call(\"rbind\", lapply(tweets, as.data.frame))\n",
    "write.csv(df, \"ca_all_tweets_28_03.csv\")\n",
    "write.table(df$text, file = \"ca_tweets_28_03.txt\", sep = \". \", row.names = FALSE)\n",
    "write.table(df$text, file = \"ca_all_tweets.txt\", append=TRUE, sep = \". \", row.names = FALSE)\n",
    "\n",
    "# References\n",
    "# https://www.rdocumentation.org/packages/twitteR/versions/1.1.9/topics/setup_twitter_oauth\n",
    "# https://www.rdocumentation.org/packages/twitteR/versions/1.1.9/topics/searchTwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to obtain aricle urls using nytimes article search api and storing them in a csv\n",
    "\n",
    "import urllib.request\n",
    "import csv\n",
    "import json\n",
    "contents = urllib.request.urlopen(\"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=******************&q=cambridge%20analytica&fl=web_url&begin_date=20180331&end_date=20180401\").read().decode('utf-8')\n",
    "json_obj = json.loads(contents)\n",
    "print(json_obj['response']['docs'])\n",
    "with open(\"ca_url_03_31.csv\", 'a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"url\",\"score\"])\n",
    "for url in json_obj['response']['docs']:\n",
    "    with open(\"ca_url_03_31.csv\", 'a', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([url['web_url'],url['score']])\n",
    "        \n",
    "# References\n",
    "# https://developer.nytimes.com/article_search_v2.json#/Console/GET/articlesearch.json\n",
    "# https://docs.python.org/3/library/urllib.request.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to read the url csv, scrape the article, get the article text and store in text files\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "df=pd.read_csv('ca_url_03_31.csv')\n",
    "article = \"\"\n",
    "for url in df[\"url\"].iteritems():\n",
    "    print(url[1])\n",
    "    session = requests.Session()\n",
    "    req = session.get(url[1])\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for p in paragraphs:\n",
    "        one = p.get_text().replace(\",\", \" \")\n",
    "        article = article + one \n",
    "file = open(\"ca_article_03_31.txt\",\"w\") \n",
    "file.write(article)\n",
    "file.close()\n",
    "file = open(\"ca_all_articles.txt\",\"a\") \n",
    "file.write(article)\n",
    "file.close()\n",
    "\n",
    "# References\n",
    "# http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to scrape the articles for cooccurance data\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "df=pd.read_csv('url_03_31.csv')\n",
    "article = \"\"\n",
    "for url in df[\"url\"].iteritems():\n",
    "    print(url[1])\n",
    "    session = requests.Session()\n",
    "    req = session.get(url[1])\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    for p in paragraphs:\n",
    "        one = p.get_text().replace(\",\", \" \")\n",
    "        article = article + one + \"\\n\\n\"\n",
    "file = open(\"sample_cooccurance.txt\",\"w\") \n",
    "file.write(article)\n",
    "file.close() \n",
    "\n",
    "# References\n",
    "# http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
